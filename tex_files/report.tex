%%%%% Beginning of preamble %%%%%

\documentclass[12pt]{article}  %What kind of document (article) and what size}

\usepackage{amssymb, amsmath, amsthm}
\usepackage{bbm}
\usepackage[margin=1in]{geometry}
\parskip = 0.1in

\title{MA 750 - Final Project}
\author{
  Nate Josephs\\
  \and
  Matthew Wiens 
  \and 
  Ben Draves
}


\begin{document}

\maketitle 



\begin{abstract} One of the most fundamental tasks in Statistics is to understand the relationship between two random variables, $X,Y$, via an unspecified function $Y = f(X)$. Typically, $f(\cdot)$ is unknown and must be estimated from data relating $X$ and $Y$. Estimating $f(\cdot)$ using maximum likelihood yields no meaningful solution when we consider \textit{all} functions. Hence statisticians turn to estimating $f\in\mathcal{F}$ where $\mathcal{F}$ is a function space with some structure that provides meaningful solutions to the problem at hand. In most cases however, these function spaces are fixed, with no regard to the sample from which we are trying to infer $f$. In order to utilize all information inherent in the data while still imposing structure on $\mathcal{F}$, \textit{Sieve Estimation} allows $\mathcal{F}$ to grow in complexity as $n$ increases. Heuristically, as $n$ increases, we attain a more robust understanding of $f$ and should allow our modeling procedure to consider more complex forms of $f$. Sieve achieves this by introducing more complex functions to $\mathcal{F}$ as $n$ increases. Here, we consider the function space $$\mathcal{F}_n = \Big\{g(x): g(x) = \sum_{d=1}^{D(n)}\beta_dx^d\Big\}$$ where $D(n)\to\infty$ as $n\to\infty$. We focus our efforts on estimating $D(n)$ as a function of the data. This report is organized as follows; in sections 1 and 2 we summarize some of the foundational results on Sieve estimation and introduce notation used throughout the report. In section 3 we introduce some theoretical applications and in section 4 we offer some methodologies of estimating $D(n)$. Lastly, in section 5 we analyze our methods via intensive simulation study and a real data application.


\end{abstract}
  
\section{Introduction}
\section{Developing Sieve Estimation}

\begin{itemize}
\item Define series estimator \textbf{(M)}

Recall that a sieve estimator for the unknown mean function $g(x)$ in the regression is a sequence of models. The motivation of sieve estimators leads to additional constraints, namely that the models should be have finite dimension, and that the complexity (i.e. dimension) increases as n increases.  Therefore the most natural choice for a sieve estimator and the choice we will focus on is a series estimator: 
$$ g_m(x) = \sum_{d=0}^{D} \alpha_{dm} \phi_{dm}(x)$$  [Hansen]
Such a formulation leads to traditional problems in linear models, so given the dimension of  the model and a choice of basis function, finding the coefficients $\alpha$ has well known solutions.  The constraints in the formulation are light; namely the choice of basis function must approximate functions in $L^2$, among other natural constraints for the regression problem.  For completeness, examples of non linear sieves are Neural Networks, or penalized sieve estimators.  [Chen]



\item Parameters - choice of basis function and dimensionality $D$ \textbf{(M)}

Given the sieve estimator defined as a series, there are two choices to be made: the dimension of the model, and the choice of basis function.  $D$ must be data dependent to scale the complexity of the estimator with the data. Therefore, for now we will assume there is an algorithm to choose $D(n)$ which produces a $D_{opt}$, for some sense of optimality. There are a number of choices for the basis function, and we will highlight just a few possibilities here: 

\begin{itemize}
\item Polynomials $$ \phi_d(x) = \sum_{i=0}^{d} c_d x^d$$
Within the polynomial class, choices of $c$ determine the exact form. For example $\textbf{c} = [0,0 \ldots, 1]$ would correspond to a basis of $1,x,x^2,x^3...$. The Hermite Polynomials would correspond to another choice of $\textbf{c}$, which have nice theoretical properties because the Hermite Polynomials are orthogonal.

\item Fourier Series

$$ \phi_d(x) = a~cos(\pi d x) + b~sin(\pi d x)$$

A polynomial basis is not a natural choice when $g(x)$ goes to zero and $\pm \infty$. Using a Fourier Series resolves this issue, which is a clear example of the problem suggesting an appropriate choice of basis function.  Another natural choice is using a Fourier basis to approximate periodic functions, and a further reduction can be made to take just the sine or cosine terms if a priori model structure warrants such a choice.

\item Gaussian 

$$  \phi_d(x) = \varphi^{(d)}(x)$$

Where $\varphi^{(d)}(x)$ is the $dth$ derivative of the Normal density, which will be the Normal density with coefficients  of the $dth$ Hermite polynomial. Like the Fourier Series, the Gaussian series estimator has the nice property that it goes to zero at $\pm \infty$.

\end{itemize}

Other common choices for the basis function are Splines and Wavelets. A univariate polynomial spline has dimension of the number of partitions plus the degree of the spline.  The choice of basis in the univariate case extends naturally to the multidimensional case, where the multivariate basis is constructed as a tensor produce of the univariate basis. [Chen]



\item Compare to Kernel Density estimates (Kernel and h) \textbf{(M)}

With the setup of a series estimator with a choice of dimension and basis function the sieve estimator has a number of similarities to the Kernel Density estimation problem.  In both cases there are two choices to be made, one parameter that controls the bias variance tradeoff, and a second parameter that is related to underlying beliefs about the model.  So the bandwidth in the Kernel estimation problem is analogous to the choice of dimension. Intuitively, as the dimension increases or the bandwidth decreases the estimator is more sensitive to local behavior and produces a rougher estimate. Similarly, the choice of the Kernel is analogous to the choice of basis function; the choice impacts the final model and exact statistical properties of the estimator, yet is less interesting because any reasonable choice of basis function or kernel function produces a similar estimate. 

\item Suppose $D$ is fixed - what is Bias, Variance, MSE, MISE \textbf{(B)}
\item Develop PSE and connect to MISE \textbf{(N)}

Instead of MISE, one may be interested in calculating the Predicted Square Error (PSE) in order to find the optimal dimension with respect to PSE. If $x^*$  is a new value from $X \sim f(x)$, then our prediction of $Y$ given $X = x^*$ under the sieve estimator is $\hat{y}^* = \hat{f}(x^*)$. We then define PSE as the expectation of the squared error between $Y^*$, the actual value of the regression line at $X=x^*$, and $\hat{y}^*$. Observe that

\begin{align*}
PSE\Big(\hat{f}(x^*)\Big) &= E\Big[(Y^* - \hat{y}^*)^2\Big] \\
&= E\Big[\Big(f(x^*) + e^* - \hat{f}(x^*)\Big)^2\Big] \\
&= E\Big[\Big(e^* + (f(x^*) - \hat{f}(x^*)\Big)^2\Big] \\
&= E\big[{e^*}^2\big] + 2E\Big[e^*\Big(f(x^*) - \hat{f}(x^*)\Big)\Big] + E\Big[\Big(f(x^*) - \hat{f}(x^*)\Big)^2\Big] \\
&= E\Big[{(e^* - E[e^*])}^2\Big] + 2E\big[e^*\big]E\Big[\Big(f(x^*) - \hat{f}(x^*)\Big)\Big] + E\Big[\Big(f(x^*) - \hat{f}(x^*)\Big)^2\Big] \\
&= Var(e^*) + 0 + \int E[(f(x) - \hat{f}(x))^2]g(x)dx \\
& = Var(e^*) + MISE\Big(\hat{f}(x)\Big)
\end{align*}

Hence, the optimal dimension for our sieve estimator with respect to PSE will be the same as the optimal dimension with respect to MISE, since minimizing PSE is equivalent to minimizing MISE. Note that in our derivation, we use the fact that the errors are zero mean and $e^*$ is independent of the estimator. 

If we define $\tilde{e} = Y^* - \hat{y}^*$, then $PSE\Big(\hat{f}(x^*)\Big) = E\big[\tilde{e}^2\big]$, which we may interpret as the expectation of a single leave-one-out (LOO) squared prediction error, where our estimator is fit on $X_1, \ldots, X_n$ and validated against $X = x^*$. This motivates us to consider LOO prediction errors for each $i = 1, \ldots, n$, which will reveal a data-driven process for choosing an optimal dimension $D$.

For each $i$, define $\tilde{e}_i = y_i - \hat{y_{(i)}}$ where $\hat{y_{(i)}}$ is fit on $X_1, \ldots, X_{i-1}, X_{i+1}, \ldots, X_n$. Then $PSE\Big(\hat{f_{(i)}}(x)\Big) = E\big[\tilde{e_i}^2\big]$ and we define the cross-validation (CV) criterion as

$$CV(\hat{f}) = \frac{1}{n}\sum_{i=1}^n \tilde{e}_i$$.

By the linearity of expectation, we have that $E\Big[CV(\hat{f})\Big] = PSE\Big(\hat{f}(x^*)\Big)$, which we will utilize as our data-driven procedure for choosing $D_{opt}$. This estimate for $MISE$ has asymptotic results that overcome the infeasibility of finding $D_{opt}$ analytically.

\end{itemize}
\section{Estimating $D(n)$}

\begin{itemize}
\item Asymptotic Optimality of CV
\item CV and other data dependent methods for choice of $D$
\item Asymptotic Behavior of Cross Validation 
\item Convergence rates
\end{itemize}

\section{Theoretical Applications}
\begin{itemize}
\item Choosing basis function sets 
\item Relationship to penalization schemes and information criterion
\end{itemize}

\section{Simulation and Applications}

\begin{itemize}
\item Simulation - effect of basis sets and optimal dimension on the project 
\item Show how $D_{opt}$ depends on $n$ (Hansen, Section 6)
\item Application 
\end{itemize}


\section{Conclusion}



\end{document} 

